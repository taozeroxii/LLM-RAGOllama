const { GoogleGenerativeAI } = require('@google/generative-ai');
const { generateEmbedding } = require('./embeddingService');
const { searchSimilarChunks, getDocumentById, getAllChunksWithEmbeddings, getImagesByDocumentId } = require('../database');

let genAI = null;

function getGenAI() {
    if (!genAI && process.env.GEMINI_API_KEY && process.env.GEMINI_API_KEY !== 'your_gemini_api_key_here') {
        genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
    }
    return genAI;
}

/**
 * Generate response using Gemini
 */
async function generateResponseGemini(prompt, context, sources) {
    const ai = getGenAI();
    if (!ai) {
        throw new Error('Gemini API key not configured');
    }

    const model = ai.getGenerativeModel({ model: 'gemini-1.5-flash' });

    const systemPrompt = `à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™ AI à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸—à¸µà¹ˆà¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¹ƒà¸™à¸à¸²à¸£à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£à¸­à¸‡à¸„à¹Œà¸à¸£ à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸ªà¸¡à¸­

## à¸«à¸¥à¸±à¸à¸à¸²à¸£à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡:
1. **à¸•à¸­à¸šà¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™** - à¸«à¹‰à¸²à¸¡à¹€à¸”à¸²à¸«à¸£à¸·à¸­à¹ƒà¸Šà¹‰à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ à¸²à¸¢à¸™à¸­à¸
2. **à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¹à¸«à¸¥à¹ˆà¸‡à¸—à¸µà¹ˆà¸¡à¸²** - à¸£à¸°à¸šà¸¸à¸Šà¸·à¹ˆà¸­à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¸•à¸­à¸š
3. **à¸•à¸­à¸šà¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™** - à¸à¸£à¸°à¸Šà¸±à¸š à¸Šà¸±à¸”à¹€à¸ˆà¸™ à¸„à¸£à¸šà¸–à¹‰à¸§à¸™
4. **à¸¢à¸­à¸¡à¸£à¸±à¸šà¹€à¸¡à¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸žà¸š** - à¸«à¸²à¸à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¹ƒà¸«à¹‰à¸šà¸­à¸à¸§à¹ˆà¸²à¹„à¸¡à¹ˆà¸žà¸šà¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£

## à¸£à¸¹à¸›à¹à¸šà¸šà¸à¸²à¸£à¸•à¸­à¸š:
- à¹ƒà¸Šà¹‰à¸ à¸²à¸©à¸²à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸—à¸²à¸‡à¸à¸²à¸£à¹à¸¥à¸°à¸ªà¸¸à¸ à¸²à¸ž
- à¸ˆà¸±à¸”à¸¥à¸³à¸”à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸«à¹‰à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸‡à¹ˆà¸²à¸¢
- à¸«à¸²à¸à¸¡à¸µà¸£à¸²à¸¢à¸à¸²à¸£à¸«à¸¥à¸²à¸¢à¸‚à¹‰à¸­ à¹ƒà¸Šà¹‰à¹€à¸¥à¸‚à¸à¸³à¸à¸±à¸š
- à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸”à¹‰à¸§à¸¢à¸£à¸¹à¸›à¹à¸šà¸š [à¸Šà¸·à¹ˆà¸­à¹€à¸­à¸à¸ªà¸²à¸£]

## à¹€à¸­à¸à¸ªà¸²à¸£à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡:
${context}

## à¸„à¸³à¸–à¸²à¸¡à¸ˆà¸²à¸à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰:
${prompt}

## à¸„à¸³à¸•à¸­à¸š (à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™ à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¹à¸«à¸¥à¹ˆà¸‡à¸—à¸µà¹ˆà¸¡à¸²):`;

    const result = await model.generateContent(systemPrompt);

    return result.response.text();
}

/**
 * Generate response using Ollama (fallback)
 */
async function generateResponseOllama(prompt, context) {
    const baseUrl = process.env.OLLAMA_BASE_URL || 'http://localhost:11434';
    const model = process.env.OLLAMA_MODEL || 'llama3.2';

    const systemPrompt = `à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™ AI à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸—à¸µà¹ˆà¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¹ƒà¸™à¸à¸²à¸£à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£à¸­à¸‡à¸„à¹Œà¸à¸£ à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸ªà¸¡à¸­

## à¸«à¸¥à¸±à¸à¸à¸²à¸£à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡:
1. à¸•à¸­à¸šà¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ - à¸«à¹‰à¸²à¸¡à¹€à¸”à¸²à¸«à¸£à¸·à¸­à¹ƒà¸Šà¹‰à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ à¸²à¸¢à¸™à¸­à¸
2. à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¹à¸«à¸¥à¹ˆà¸‡à¸—à¸µà¹ˆà¸¡à¸² - à¸£à¸°à¸šà¸¸à¸Šà¸·à¹ˆà¸­à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¸à¸²à¸£à¸•à¸­à¸š
3. à¸•à¸­à¸šà¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™ - à¸à¸£à¸°à¸Šà¸±à¸š à¸Šà¸±à¸”à¹€à¸ˆà¸™ à¸„à¸£à¸šà¸–à¹‰à¸§à¸™
4. à¸¢à¸­à¸¡à¸£à¸±à¸šà¹€à¸¡à¸·à¹ˆà¸­à¹„à¸¡à¹ˆà¸žà¸š - à¸«à¸²à¸à¹„à¸¡à¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¹ƒà¸«à¹‰à¸šà¸­à¸à¸§à¹ˆà¸²à¹„à¸¡à¹ˆà¸žà¸šà¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£

## à¹€à¸­à¸à¸ªà¸²à¸£à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡:
${context}

## à¸„à¸³à¸–à¸²à¸¡à¸ˆà¸²à¸à¸œà¸¹à¹‰à¹ƒà¸Šà¹‰:
${prompt}

## à¸„à¸³à¸•à¸­à¸š (à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¸•à¸£à¸‡à¸›à¸£à¸°à¹€à¸”à¹‡à¸™ à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡à¹à¸«à¸¥à¹ˆà¸‡à¸—à¸µà¹ˆà¸¡à¸²):`;

    const response = await fetch(`${baseUrl}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
            model: model,
            prompt: systemPrompt,
            stream: false,
            options: {
                temperature: 0.3,  // Lower temperature for more accurate responses
                num_ctx: 4096      // Larger context window
            }
        })
    });

    if (!response.ok) {
        throw new Error(`Ollama generate failed: ${response.statusText}`);
    }

    const data = await response.json();
    return data.response;
}

/**
 * Generate response with fallback
 */
async function generateResponse(prompt, context) {
    const provider = process.env.LLM_PROVIDER || 'auto';

    if (provider === 'gemini' || provider === 'auto') {
        try {
            return await generateResponseGemini(prompt, context);
        } catch (error) {
            console.warn('Gemini generate failed:', error.message);
            if (provider === 'auto') {
                console.log('Falling back to Ollama for generation...');
                try {
                    return await generateResponseOllama(prompt, context);
                } catch (ollamaError) {
                    console.warn('Ollama also failed:', ollamaError.message);
                    // Return a basic response with the context
                    return generateBasicResponse(prompt, context);
                }
            }
            throw error;
        }
    } else if (provider === 'ollama') {
        try {
            return await generateResponseOllama(prompt, context);
        } catch (error) {
            console.warn('Ollama generate failed:', error.message);
            return generateBasicResponse(prompt, context);
        }
    }

    throw new Error(`Unknown LLM provider: ${provider}`);
}

/**
 * Generate a basic response when LLM is unavailable
 */
function generateBasicResponse(prompt, context) {
    return `ðŸ“Œ **à¸£à¸°à¸šà¸š AI à¹„à¸¡à¹ˆà¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸Šà¸±à¹ˆà¸§à¸„à¸£à¸²à¸§** (API quota à¸«à¸¡à¸”à¸«à¸£à¸·à¸­ Ollama à¹„à¸¡à¹ˆà¸—à¸³à¸‡à¸²à¸™)

à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸à¹‡à¸•à¸²à¸¡ à¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£:

---
${context.substring(0, 1500)}${context.length > 1500 ? '...' : ''}
---

ðŸ’¡ **à¸§à¸´à¸˜à¸µà¹à¸à¹‰à¹„à¸‚:**
- à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š Gemini API quota à¸—à¸µà¹ˆ https://ai.google.dev/
- à¸«à¸£à¸·à¸­à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Ollama: https://ollama.ai/ à¹à¸¥à¹‰à¸§à¸£à¸±à¸™: ollama pull llama3.2`;
}

/**
 * Search chunks using keyword matching as fallback
 */
function searchChunksByKeyword(question, limit = 5) {
    const chunks = getAllChunksWithEmbeddings();
    const keywords = question.toLowerCase().split(/\s+/).filter(k => k.length > 2);

    const scoredChunks = chunks.map(chunk => {
        const content = chunk.content.toLowerCase();
        let score = 0;
        for (const keyword of keywords) {
            if (content.includes(keyword)) {
                score += 1;
            }
        }
        return { ...chunk, similarity: score / Math.max(keywords.length, 1) };
    });

    return scoredChunks
        .filter(c => c.similarity > 0)
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, limit);
}

/**
 * Main RAG pipeline - retrieve relevant chunks and generate answer
 */
async function ragQuery(question) {
    let similarChunks = [];
    let useKeywordFallback = false;

    // 1. Try to generate embedding for the question
    try {
        const queryEmbedding = await generateEmbedding(question);
        similarChunks = searchSimilarChunks(queryEmbedding, 5);
    } catch (embeddingError) {
        console.warn('Embedding failed, using keyword search:', embeddingError.message);
        useKeywordFallback = true;
        similarChunks = searchChunksByKeyword(question, 5);
    }

    // 2. If no results with embedding, try keyword search
    if (similarChunks.length === 0 && !useKeywordFallback) {
        console.log('No embedding results, trying keyword search...');
        similarChunks = searchChunksByKeyword(question, 5);
    }

    if (similarChunks.length === 0) {
        return {
            answer: 'à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¹ƒà¸™à¹€à¸­à¸à¸ªà¸²à¸£ à¸à¸£à¸¸à¸“à¸²à¸¥à¸­à¸‡à¸–à¸²à¸¡à¸„à¸³à¸–à¸²à¸¡à¸­à¸·à¹ˆà¸™ à¸«à¸£à¸·à¸­à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µà¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¹„à¸”à¹‰à¸–à¸¹à¸à¸­à¸±à¸žà¹‚à¸«à¸¥à¸”à¹à¸¥à¹‰à¸§',
            sources: []
        };
    }

    // 3. Build context from chunks
    const contextParts = similarChunks.map((chunk, index) => {
        return `[à¹€à¸­à¸à¸ªà¸²à¸£ ${index + 1}: ${chunk.document_name}]\n${chunk.content}`;
    });
    const context = contextParts.join('\n\n---\n\n');

    // 4. Generate response (with fallback)
    const answer = await generateResponse(question, context);

    // 5. Prepare sources with document info and images
    const sourceDocIds = [...new Set(similarChunks.map(c => c.document_id))];
    const sources = sourceDocIds.map(docId => {
        const doc = getDocumentById(docId);
        const relevantChunks = similarChunks.filter(c => c.document_id === docId);
        const maxSimilarity = Math.max(...relevantChunks.map(c => c.similarity));

        // Get images for this document
        const docImages = getImagesByDocumentId(docId);
        const images = docImages.map(img => ({
            id: img.id,
            url: `/uploads/images/${img.filename}`,
            alt: img.alt_text || `Image from ${doc?.original_name || 'document'}`
        }));

        return {
            documentId: docId,
            documentName: doc?.original_name || 'Unknown',
            relevance: Math.round(maxSimilarity * 100),
            preview: relevantChunks[0]?.content.substring(0, 150) + '...',
            images: images
        };
    });

    // Collect all images from all sources
    const allImages = sources.flatMap(s => s.images || []);

    return {
        answer,
        sources,
        images: allImages.slice(0, 5) // Limit to 5 images max
    };
}

module.exports = {
    ragQuery,
    generateResponse
};

